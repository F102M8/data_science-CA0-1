{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecfbb4b0c704e67",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Web Scraping and Introductory Data Analysis\n",
    "\n",
    "Welcome to Homework 0, where we will delve into web scraping and perform an introductory data analysis. This homework will be a hands-on exercise that will help you become familiar with the process of extracting data from websites and conducting basic statistical analysis. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this homework, you will be able to:\n",
    "\n",
    "1. Set up a Python environment with the necessary libraries for web scraping and data analysis.\n",
    "2. Write a web scraping script using Beautiful Soup and Selenium to collect data from a website.\n",
    "3. Sample from the collected dataset and compare the statistics of the sample and the population.\n",
    "   \n",
    "## Tasks\n",
    "\n",
    "1. **Environment Setup**: Install the required libraries such as Beautiful Soup, Selenium, pandas, numpy, matplotlib, and seaborn.\n",
    "\n",
    "2. **Web Scraping**: Write a script to scrape transaction data from [Etherscan.io](https://etherscan.io/txs). Use Selenium to interact with the website and Beautiful Soup to parse the HTML content.\n",
    "\n",
    "3. **Data Sampling**: Once the data is collected, create a sample from the dataset. Compare the sample statistics (mean and standard deviation) with the population statistics.\n",
    "\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. A Jupyter notebook with all the code and explanations.\n",
    "2. A detailed report on the findings, including the comparison of sample and population statistics.\n",
    "Note: You can include the report in your notebook.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "Begin by setting up your Python environment and installing the necessary libraries. Then, proceed with the web scraping task, ensuring that you handle any potential issues such as rate limiting. Once you have the data, move on to the data sampling and statistical analysis tasks. \n",
    "\n",
    "Remember to document your process and findings in the Jupyter notebook, and to include visualizations where appropriate to illustrate your results. <br>\n",
    "Good luck, and happy scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca352a49724d191",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Collection (Etherscan)\n",
    "\n",
    "In this section, we will use web scraping to gather transaction data from the Ethereum blockchain using the Etherscan block explorer. Our objective is to collect transactions from the **last 10 blocks** on Ethereum.\n",
    "\n",
    "To accomplish this task, we will employ web scraping techniques to extract the transaction data from the Etherscan website. The URL we will be targeting for our data collection is:\n",
    "\n",
    "[https://etherscan.io/txs](https://etherscan.io/txs)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Navigate to the URL**: Use Selenium to open the Etherscan transactions page in a browser.\n",
    "\n",
    "2. **Locate the Transaction Data**: Identify the HTML elements that contain the transaction data for the specified block range.\n",
    "\n",
    "3. **Extract the Data**: Write a script to extract the transaction details e.g. Hash, Method, Block, etc.\n",
    "\n",
    "4. **Handle Pagination**: If the transactions span multiple pages, implement pagination handling to navigate through the pages and collect all relevant transaction data.\n",
    "\n",
    "5. **Store the Data**: Save the extracted transaction data into a structured format, such as a CSV file or a pandas DataFrame, for further analysis.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Rate Limiting**: Be mindful of the website's rate limits to avoid being blocked. Implement delays between requests if necessary.\n",
    "- **Dynamic Content**: The Etherscan website may load content dynamically. Ensure that Selenium waits for the necessary elements to load before attempting to scrape the data.\n",
    "- **Data Cleaning**: After extraction, clean the data to remove any inconsistencies or errors that may have occurred during the scraping process.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Selenium Documentation](https://selenium-python.readthedocs.io/)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Ethereum](https://ethereum.org/en/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c8268-ee5c-4210-9e04-c449c744839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas matplotlib seaborn scipy selenium webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa10db-ec9e-4921-870a-50066926ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6036a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "chrome_service = Service(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f94ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d706fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://etherscan.io/blocks'\n",
    "chrome_options = Options()\n",
    "\n",
    "dr = webdriver.Chrome(options=chrome_options, service=chrome_service)\n",
    "\n",
    "WebDriverWait(dr, 10).until(lambda dr: dr.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "dr.get(url)\n",
    "page = dr.page_source\n",
    "dr.quit()\n",
    "    \n",
    "\n",
    "soup = BeautifulSoup(page, 'html')\n",
    "table = soup.find('table')\n",
    "\n",
    "tbody = table.find('tbody')\n",
    "trs = tbody.find_all('tr')\n",
    "\n",
    "block_numbers = []\n",
    "for tr in trs[:10]:\n",
    "    td = tr.find('td')\n",
    "    block_number = td.text.strip()\n",
    "    block_numbers.append(block_number)\n",
    "    \n",
    "block_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "df = None\n",
    "for block_number in block_numbers:\n",
    "    p = 1\n",
    "    while True:\n",
    "        url = f'https://etherscan.io/txs?block={block_number}&p={p}'\n",
    "        sleep(0.5)\n",
    "        chrome_options = Options()\n",
    "\n",
    "        dr = webdriver.Chrome(options=chrome_options, service=chrome_service)\n",
    "\n",
    "        WebDriverWait(dr, 10).until(lambda dr: dr.execute_script('return document.readyState') == 'complete')\n",
    "\n",
    "        dr.get(url)\n",
    "        page = dr.page_source\n",
    "        dr.quit()        \n",
    "        \n",
    "        soup = BeautifulSoup(page, 'html')\n",
    "        table = soup.find('table')\n",
    "        \n",
    "        if df is None:\n",
    "            thead = table.find('thead')\n",
    "            thead_titles = thead.find_all('th')\n",
    "            titles = [table_title.text.strip() for table_title in thead_titles]\n",
    "            df = pd.DataFrame(columns=titles)\n",
    "        \n",
    "        tbody = table.find('tbody')\n",
    "        trs = tbody.find_all('tr')\n",
    "        if len(trs) == 1: \n",
    "            break\n",
    "        \n",
    "        for tr in trs:\n",
    "            tds = tr.find_all('td')\n",
    "            tds = filter(lambda td: not (td.get('style') and 'display:none' in td.get('style')), tds)\n",
    "            data = [td.text.strip() for td in tds]\n",
    "\n",
    "            length = len(df)\n",
    "            df.loc[length] = data\n",
    "        p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bfd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdddddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a013b104d142cfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Analysis\n",
    "\n",
    "Now that we have collected the transaction data from Etherscan, the next step is to perform conduct an initial analysis. This task will involve the following steps:\n",
    "\n",
    "1. **Load the Data**: Import the collected transaction data into a pandas DataFrame.\n",
    "\n",
    "2. **Data Cleaning**: Clean the data by converting data types, removing any irrelevant information, and handling **duplicate** values.\n",
    "\n",
    "3. **Statistical Analysis**: Calculate the mean and standard deviation of the population. Evaluate these statistics to understand the distribution of transaction values. The analysis and plotting will be on **Txn Fee** and **Value**.\n",
    "\n",
    "4. **Visualization**: This phase involves the creation of visual representations to aid in the analysis of transaction values. The visualizations include:\n",
    "    - A histogram for each data column, which provides a visual representation of the data distribution. The selection of bin size is crucial and should be based on the data's characteristics to ensure accurate representation. Provide an explanation on the bin size selection!\n",
    "    - A normal distribution plot fitted alongside the histogram to compare the empirical distribution of the data with the theoretical normal distribution.\n",
    "    - A box plot and a violin plot to identify outliers and provide a comprehensive view of the data's distribution.\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "The project aims to deliver the following deliverables:\n",
    "\n",
    "- A refined pandas DataFrame containing the transaction data, which has undergone thorough cleaning and is ready for analysis.\n",
    "- A simple statistical analysis evaluating the population statistics, offering insights into the distribution of transaction values and fees.\n",
    "- A set of visualizations showcasing the distribution of transaction values for the population. These visualizations include histograms, normal distribution plots, box plots, and violin plots, each serving a specific purpose in the analysis.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "The project starts with the importing of transaction data into a pandas DataFrame, setting the stage for data manipulation and analysis. Subsequent steps involve the cleaning of the data to ensure its quality and reliability. Followed by the calculation of population statistics. Finally, a series of visualizations are created to visually analyze the distribution of transaction values and fees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481b11a08d876b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T14:02:12.152030482Z",
     "start_time": "2024-02-25T14:02:12.101846096Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Txn Fee'] = pd.to_numeric(df['Txn Fee'], errors='coerce')\n",
    "filtered_df = df[df['Value'].str.endswith('ETH')]\n",
    "filtered_df['Value'] = filtered_df['Value'].str.replace(' ETH', '').astype(float)\n",
    "df.drop([], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.drop_duplicates(inplace=True)\n",
    "filtered_df.dropna(subset=['Txn Fee', 'Value'], inplace=True)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d248b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_fee_mean = filtered_df['Txn Fee'].mean()\n",
    "txn_fee_std = filtered_df['Txn Fee'].std()\n",
    "\n",
    "value_mean = filtered_df['Value'].mean()\n",
    "value_std = filtered_df['Value'].std()\n",
    "\n",
    "print(f\"Txn Fee Mean: {txn_fee_mean}, Standard Deviation: {txn_fee_std}\")\n",
    "print(f\"Value Mean: {value_mean}, Standard Deviation: {value_std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0877c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_and_normal_dist(column, bin_size='auto', title=''):\n",
    "    sns.histplot(filtered_df[column], bins=bin_size, kde=False, stat='density', label='Histogram')\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, filtered_df[column].mean(), filtered_df[column].std())\n",
    "    plt.plot(x, p, 'k', linewidth=2, label='Normal dist')\n",
    "    title = title or f'Distribution of {column}'\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_histogram_and_normal_dist('Txn Fee', title='Txn Fee Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_histogram_and_normal_dist('Value', title='Value Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=filtered_df, y='Txn Fee')\n",
    "plt.title('Box Plot of Txn Fee')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.violinplot(data=filtered_df, y='Value')\n",
    "plt.title('Violin Plot of Value')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87030e5e0b4fe1e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Sampling and Analysis\n",
    "\n",
    "In this section, we will delve into the process of data sampling and perform an initial analysis on the transaction data we have collected. Our objective is to understand the distribution of transaction values by sampling the data and comparing the sample statistics with the population statistics.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Load the Data**: Import the collected transaction data into a pandas DataFrame.\n",
    "\n",
    "2. **Data Cleaning**: Clean the data by handling missing values, converting data types, and removing any irrelevant information.\n",
    "\n",
    "3. **Simple Random Sampling (SRS)**: Create a sample from the dataset using a simple random sampling method. This involves randomly selecting a subset of the data without regard to any specific characteristics of the data.\n",
    "\n",
    "4. **Stratified Sampling**: Create another sample from the dataset using a stratified sampling method. This involves dividing the data into strata based on a specific characteristic (e.g., transaction value) and then randomly selecting samples from each stratum. Explain what you have stratified the data by and why you chose this column.\n",
    "\n",
    "5. **Statistical Analysis**: Calculate the mean and standard deviation of the samples and the population. Compare these statistics to understand the distribution of transaction values.\n",
    "\n",
    "6. **Visualization**: Plot the distribution of transaction values and fees for both the samples and the population to visually compare their distributions.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Sample Size**: The size of the sample should be large enough to represent the population accurately but not so large that it becomes impractical to analyze.\n",
    "- **Sampling Method**: Choose the appropriate sampling method based on the characteristics of the data and the research question.\n",
    "\n",
    "Explain the above considerations in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6becd41f9b2393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T19:36:26.333480965Z",
     "start_time": "2024-02-27T19:36:26.324023052Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the Data:\n",
    "transactions_df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1463b1b-2cf2-4b37-ad4a-a21ae81f24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58e60c-de14-4c22-b8da-6845aac5f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data:\n",
    "\n",
    "# Handling missing values:\n",
    "transactions_df.dropna(subset=['Txn Fee', 'Value'], inplace=True)\n",
    "\n",
    "# Converting data types:\n",
    "transactions_df['Txn Fee'] = pd.to_numeric(transactions_df['Txn Fee'], errors='coerce')\n",
    "filtered_transactions_df = transactions_df[transactions_df['Value'].str.endswith('ETH')]\n",
    "filtered_transactions_df['Value'] = filtered_transactions_df['Value'].str.replace(' ETH', '').astype(float)\n",
    "filtered_transactions_df.drop([], axis=1, inplace=True)\n",
    "\n",
    "# Removing any irrelevant information:\n",
    "filtered_transactions_df = filtered_transactions_df.loc[:, ~filtered_transactions_df.columns.str.match('Unnamed: ')]\n",
    "filtered_transactions_df.drop(['Txn Hash','Method', 'Block', 'Age', 'From', 'To'], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c532e61-2e1a-41cb-9b4e-63e3774a9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae27565-6e16-4569-8ba8-31b8e58009d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Random Sampling:\n",
    "sample_size = 100 \n",
    "\n",
    "if filtered_transactions_df.empty:\n",
    "    print(\"The DataFrame is empty.\")\n",
    "else:\n",
    "    print(f\"The DataFrame has {len(filtered_transactions_df)} rows.\")\n",
    "if len(filtered_transactions_df) < sample_size:\n",
    "    print(f\"Cannot sample {sample_size} rows from a DataFrame with only {len(filtered_transactions_df)} rows.\")\n",
    "else:\n",
    "    SRS_samples = filtered_transactions_df.sample(n=sample_size)\n",
    "    print(\"Sampled successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10a668-62bd-46dd-85cf-bb0c20789b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRS_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed715edd-aa76-4862-ae6c-d1c7e5e6ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25c181-e380-4858-9ad7-6be1aa027850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis:\n",
    "\n",
    "# ***SRS***\n",
    "SRS_txn_fee_mean = SRS_samples['Txn Fee'].mean()\n",
    "SRS_txn_fee_std = SRS_samples['Txn Fee'].std()\n",
    "\n",
    "SRS_value_mean = SRS_samples['Value'].mean()\n",
    "SRS_value_std = SRS_samples['Value'].std()\n",
    "\n",
    "print(f\"SRC: Txn Fee Mean: {txn_fee_mean}, Standard Deviation: {txn_fee_std}\")\n",
    "print(f\"SRC: Value Mean: {value_mean}, Standard Deviation: {value_std}\")\n",
    "\n",
    "# ***Stratified Sampling***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c3f71-f77e-4c3b-86e9-759b00b304f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
